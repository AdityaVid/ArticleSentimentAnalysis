{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02834489-2d65-4c98-a1d0-4aa8304d8c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from os import listdir\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import trafilatura\n",
    "import lxml_html_clean as cl\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "stopwords=[]\n",
    "current_directory = os.getcwd()\n",
    "pathname = os.path.join(current_directory, \"data/StopWords_Auditor.txt\")\n",
    "\n",
    "\n",
    "with open(pathname,'r') as file:\n",
    "    #print(file)\n",
    "    for line in file:\n",
    "        #print(line)\n",
    "        for word in re.findall(r'\\w+', line):\n",
    "            stopwords.append(word.lower())\n",
    "            \n",
    "                        \n",
    "pathname = os.path.join(current_directory, \"data/StopWords_Currencies.txt\")\n",
    "\n",
    "with open(pathname,'r') as file:\n",
    "    #print(file)\n",
    "    for line in file:\n",
    "        #print(line)\n",
    "        for word in re.findall(r'\\w+', line):\n",
    "            stopwords.append(word.lower())\n",
    "\n",
    "\n",
    "pathname = os.path.join(current_directory, \"data/StopWords_DatesandNumbers.txt\")\n",
    "\n",
    "\n",
    "with open(pathname,'r') as file:\n",
    "    #print(file)\n",
    "    for line in file:\n",
    "        #print(line)\n",
    "        for word in re.findall(r'\\w+', line):\n",
    "            stopwords.append(word.lower())\n",
    "\n",
    "pathname = os.path.join(current_directory, \"data/StopWords_Generic.txt\")\n",
    "\n",
    "\n",
    "with open(pathname,'r') as file:\n",
    "    #print(file)\n",
    "    for line in file:\n",
    "        #print(line)\n",
    "        for word in re.findall(r'\\w+', line):\n",
    "            stopwords.append(word.lower())\n",
    "            \n",
    "pathname = os.path.join(current_directory, \"data/StopWords_GenericLong.txt\")\n",
    "\n",
    "\n",
    "with open(pathname,'r') as file:\n",
    "    #print(file)\n",
    "    for line in file:\n",
    "        #print(line)\n",
    "        for  word in re.findall(r'\\w+', line):\n",
    "            stopwords.append(word.lower())\n",
    "            \n",
    "pathname = os.path.join(current_directory, \"data/StopWords_Geographic.txt\")\n",
    "\n",
    "\n",
    "with open(pathname,'r') as file:\n",
    "    #print(file)\n",
    "    for line in file:\n",
    "        #print(line)\n",
    "        for word in re.findall(r'\\w+', line):\n",
    "            stopwords.append(word.lower())\n",
    "\n",
    "pathname = os.path.join(current_directory, \"data/StopWords_Names.txt\")\n",
    "\n",
    "\n",
    "with open(pathname,'r') as file:\n",
    "    #print(file)\n",
    "    for line in file:\n",
    "        #print(line)\n",
    "        for word in re.findall(r'\\w+', line):\n",
    "            stopwords.append(word.lower())\n",
    "\n",
    "negwords=[]\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "pathname = os.path.join(current_directory, \"data/negative-words.txt\")\n",
    "\n",
    "\n",
    "with open(pathname,'r') as file:\n",
    "  \n",
    "    for line in file:\n",
    "      \n",
    "        for word in re.findall(r'\\w+', line):\n",
    "            negwords.append(word.lower())\n",
    "#print(negwords)\n",
    "\n",
    "poswords=[]\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "pathname = os.path.join(current_directory, \"data/positive-words.txt\")\n",
    "\n",
    "\n",
    "with open(pathname,'r') as file:\n",
    "   \n",
    "    for line in file:\n",
    "       \n",
    "        for word in re.findall(r'\\w+', line):\n",
    "            poswords.append(word.lower())\n",
    "#print(poswords)\n",
    "\n",
    "#find if overlapping words\n",
    "\n",
    "neg = set(negwords)\n",
    "stop= set(stopwords)\n",
    "pos = set(poswords)\n",
    "\n",
    "#print(len(negwords)) #if overlapping in stop and negative remove from negative\n",
    "intersected_set = stop.intersection(neg)\n",
    "neg.discard(intersected_set)\n",
    "negwords=list(neg)\n",
    "#print(len(negwords))\n",
    "\n",
    "\n",
    "#print(len(poswords))-- #if overlapping in negative and positive remove from positive\n",
    "intersected_set = neg.intersection(pos)\n",
    "pos.discard(intersected_set)\n",
    "poswords=list(pos)\n",
    "#print(len(poswords))\n",
    "\n",
    "#print(len(poswords)) #if overlapping in stop and positive remove from positive\n",
    "intersected_set = stop.intersection(pos)\n",
    "pos.discard(intersected_set)\n",
    "poswords=list(pos)\n",
    "#print(len(poswords))\n",
    "\n",
    "\n",
    "                        \n",
    "\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "#open Input excel file , iterate through each row , extract the article and save to a text file saved with urlid\n",
    "\n",
    "pathname = os.path.join(current_directory, \"data\\Input.xlsx\")\n",
    "excel=pd.read_excel(pathname)\n",
    "urllst=[]\n",
    "\n",
    "for key, row in excel.iterrows(): \n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    urllst.append([url_id,url])\n",
    "    requests.get(url)\n",
    "    response = requests.get(url)  \n",
    "    if response.status_code == 200:\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        wordlist=\"\"\n",
    "        element_list=\"\"\n",
    "        article=\"\"\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        #time.sleep(5)\n",
    "        element_list=trafilatura.extract(downloaded)\n",
    "# loop through word elements to remove speacial characters like new line\n",
    "        for element in element_list:\n",
    "            wordlist+=element\n",
    "\n",
    "        article=wordlist\n",
    "        title=soup.find('title').text\n",
    "        \n",
    "        file_name = current_directory + '/data/articles/'+( url_id) + '.txt'\n",
    "        with open(file_name, 'w',encoding='utf-8') as file:\n",
    "            file.write(title + '\\n' + article)\n",
    "    else:\n",
    "        file_name = current_directory + '/data/articles/'+( url_id) + '.txt'\n",
    "        with open(file_name, 'w',encoding='utf-8') as file:\n",
    "            file.write(\"\")\n",
    "            \n",
    "urldf=pd.DataFrame(urllst,columns=['urlid','url'])     \n",
    "\n",
    "def getscores(flname,path):\n",
    "    \n",
    " #function that will open txt file score the article and prepare a list with accumulated score\n",
    "\n",
    "    file_name = path + '/'+ flname\n",
    "    filew = open(file_name, 'r',encoding='utf-8') \n",
    "    filep = open(file_name, 'r',encoding='utf-8') \n",
    "    print(file_name)\n",
    "    search=word_tokenize(filew.read())\n",
    "    para= sent_tokenize(filep.read())\n",
    "    \n",
    "    \n",
    "    cleanword=0      # word count\n",
    "    allword=0\n",
    "    l=0              # line length\n",
    "    s=0              # sentence count\n",
    "    tl=0             # total line length\n",
    "    prncnt=0\n",
    "    wordlen=0\n",
    "    positivescore=0\n",
    "    negativescore=0\n",
    "    totalsyllcnt=0\n",
    "    lowersrch=[]\n",
    "    nwsrch=[]\n",
    "    compwords=[]\n",
    "    polarityscore=0\n",
    "    subjectivescore=0\n",
    "    avgwrdperln=0\n",
    "    fogindex=0\n",
    "    syllcntperword=0\n",
    "    avgwrdlen=0\n",
    "    urlid=flname.split('.')[0]\n",
    "    \n",
    "    if len(search) == 0:\n",
    "        print('No data in file ' + file_name)\n",
    "        result.append([urlid, file_name,positivescore,negativescore,polarityscore,subjectivescore,fogindex,avgwrdperln,len(compwords),cleanword,syllcntperword,prncnt,avgwrdlen])\n",
    "        return\n",
    "\n",
    "    for x in para:\n",
    "        l=len(x)\n",
    "        s=s+1\n",
    "        tl=tl+l\n",
    "\n",
    "# Calculate complex word count, Clean word count, Pronoun word count- pronoun list expanded\n",
    "\n",
    "    try:\n",
    "        for word in search:\n",
    "            word = re.sub('[^A-Za-z0-9]+', ' ', word)\n",
    "            vowels = 'aeiou'\n",
    "            syllcnt = sum( 1 for letter in word if letter.lower() in vowels)\n",
    "            allword=allword+1\n",
    "            wordlen=wordlen+len(word)\n",
    "            totalsyllcnt=totalsyllcnt+syllcnt\n",
    "            if syllcnt > 2:\n",
    "                compwords.append(word)\n",
    "            if word.casefold() not in stopwords:\n",
    "                cleanword=cleanword+1\n",
    "                nwsrch.append(word)\n",
    "            lowersrch = list(map(lambda x : x.lower(), nwsrch))\n",
    "            pronouns=['i','we','us','my','our','ours','they','them','him','her']\n",
    "            if word.lower() in pronouns : prncnt=prncnt+1\n",
    "    except:\n",
    "           print('No Compound/Clean/Pronoun words ' + file_name) \n",
    "\n",
    "\n",
    "# Calculate positive score, negative score\n",
    "\n",
    "    neg = set(negwords)\n",
    "    stop= set(stopwords)\n",
    "    pos = set(poswords)\n",
    "    srch= set(lowersrch)\n",
    "\n",
    "    \n",
    "    try:\n",
    "        intersected_set =pos.intersection(srch)\n",
    "        intvalue=list(intersected_set )\n",
    "        c = Counter(lowersrch)\n",
    "        itemgetter(*intvalue)(c)\n",
    "        positivescore=sum(itemgetter(*intvalue)(c))\n",
    "    except:\n",
    "        print('No Positive words ' + file_name)\n",
    "        positivescore=0\n",
    "\n",
    "    try:\n",
    "        intersected_set =neg.intersection(srch)\n",
    "        intvalue=list(intersected_set )\n",
    "        c = Counter(lowersrch)\n",
    "        #print(c)\n",
    "        itemgetter(*intvalue)(c)\n",
    "        #print(intersected_set,itemgetter(*intvalue)(c))\n",
    "        negativescore=sum(itemgetter(*intvalue)(c))\n",
    "    except:\n",
    "        print('No negative words ' + file_name)\n",
    "        negativescore=0\n",
    "\n",
    "#Other Calculated fields\n",
    "    \n",
    "    wordcnt=allword\n",
    "    percompwords=len(compwords)/allword*100\n",
    "    #positivescore\n",
    "    #negativescore\n",
    "    polarityscore=(positivescore-negativescore)/(positivescore+negativescore+0.000001)\n",
    "    subjectivescore=(positivescore+negativescore)/(allword+0.000001)\n",
    "    avgwrdperln=allword/s\n",
    "    fogindex=0.4*(avgwrdperln+percompwords)\n",
    "    #compwords\n",
    "    #cleanword\n",
    "    syllcntperword=totalsyllcnt/allword\n",
    "    #prncnt\n",
    "    avgwrdlen=wordlen/allword\n",
    "    \n",
    "    \n",
    "    result.append([urlid, file_name,positivescore,negativescore,polarityscore,subjectivescore,fogindex,avgwrdperln,len(compwords),cleanword,syllcntperword,prncnt,avgwrdlen])\n",
    "    \n",
    "    return \n",
    "\n",
    "result=[]\n",
    "df = pd.DataFrame()\n",
    "current_directory = os.getcwd()\n",
    "path=current_directory+'/data/articles' \n",
    "for filename in listdir(path):\n",
    "    getscores(filename,path)\n",
    "df=pd.DataFrame(result,columns=['urlid','TxtFile Location','Positive Score','Negative Score','Polarity', 'Subjectivity','Analysis of Readability','Average No.of Words/Sentence','Complex Word Count','Word Count', 'Syllable Count/Word', 'Personal Pronouns', 'Avg Word Len'])\n",
    "\n",
    "Output=pd.merge(df,urldf, how='inner',indicator=True, left_on='urlid',right_on='urlid')\n",
    "\n",
    "savefilename=current_directory+'/data/Output.csv'\n",
    "Output.to_csv(savefilename, sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
